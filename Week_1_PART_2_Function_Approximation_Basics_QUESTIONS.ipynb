{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilianw/Microservices-With-Spring-Student-Files/blob/master/Week_1_PART_2_Function_Approximation_Basics_QUESTIONS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFuV20LKc0h3"
      },
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "# Week 1: Function Approximation Basics: Regression\n",
        "\n",
        "### Function Approximation: Your new superpower\n",
        "This project introduces the initial, core set of tools for supervised machine learning (ML). We will use simple tools to load and preprocess datasets in order to focus on the mechanics of creating and using ML models. In this first project, our dataset will be more of a \"black box,\" and later projects will bring together datasets with business context, data cleaning, and ML modeling choices.\n",
        "\n",
        "Our project will walk through problem formulation, developing models, and evaluating model performance for _regression_ (supervised learning with a continuous output value). We will introduce the basics of formalizing supervised learning as a task along with models and metrics for regression.\n",
        "\n",
        "Throughout the course, we will build and test models using `scikit-learn` and supporting tools. These tools are often the default starting point for data scientists and ML engineers doing high-stakes analysis or building towards production applications of ML models. The thought process and ML development steps in this project also apply to larger, more complex ML systems. _The core steps we introduce in the project are the same steps you will use for most new supervised learning projects!_\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. This is the ONLY required notebook for Project 1. Other notebooks are optional to help you learn basics, or go deeper. You should complete and submit this notebook for review.\n",
        "2. We provide starter code and data to give your work a common starting point and scaffolding. You must keep function signatures unchanged to support any later usage and to ensure correct grading of your project. We mark blocks where you need to add code, and you should not need to modify starting code we provide.\n",
        "3. Ensure you read through the document and starting code before beginning your work. Understand the overall structure and goals of the project to make your implementation efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TWogdWh1MWp"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGpYPUn2uVBK"
      },
      "source": [
        "Import all the necessary libraries we need throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSDbwEoL8-0r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model, svm\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
        "from numpy.core.numeric import Inf\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "\n",
        "# Testing\n",
        "%pip install -U ipytest\n",
        "import ipytest\n",
        "import pytest\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riBtB6GDur-W"
      },
      "source": [
        "Set the random seed for NumPy so that we produce consistent results that can be easily discussed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7pOF_7XXQ6M"
      },
      "outputs": [],
      "source": [
        "# Fix the random seed so that we get consistent results\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKLaamGv74gO"
      },
      "source": [
        "# Predicting diamond price with regression models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_YhkBSe9Gj"
      },
      "source": [
        "Now let's connect the supervised ML formal task definitions to how we build ML models in code. We will introduce regression modeling using a simple dataset from [OpenML](https://www.openml.org/search?type=data&sort=runs&id=42225&status=active). We will use the dataset, called `diamonds`, to predict the prices of diamonds ($y$) from its qualities ($x$). We formalize this regression task as:\n",
        "* Output $y$: Price of a diamond\n",
        "* Inputs (also called *features*) $x$: 6 attributes describing a diamond, including carat weight, dimensions, etc.\n",
        "\n",
        "Now we will load the dataset and build an initial model $f_θ(x) → y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX3VPazF8AGN"
      },
      "source": [
        "## Loading the diamonds  dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A7ren4ZnHV-"
      },
      "source": [
        "The dataset is stored in Google Drive. We will download the dataset using `gdown` and load it from a `.npz` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9huLARxwbxc"
      },
      "outputs": [],
      "source": [
        "!gdown 1lrU9Uodb09RgnVkNEYDhe4OwEb1E2VIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7T6VQ0FbLKD"
      },
      "outputs": [],
      "source": [
        "with np.load('diamonds.npz') as npdata:\n",
        "  X = npdata['X']\n",
        "  y = npdata['y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvb7kppv-2bA"
      },
      "source": [
        "## Basic dataset information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nToESDWKvzD"
      },
      "source": [
        "First, if you aren't already familiar with a dataset as a _matrix_, let's look! A dataset is collection of examples. Each example is represented by a set of numbers in an _input vector_ or a _feature vector_. We represent a collection of vectors as a 2-D _matrix_ which is the same as a table you might see in a spreadsheet.\n",
        "\n",
        "Recall that we are trying to build an ML model $f_θ(x) → y$ where the input $x$ is a vector (a list of numbers). We can then represent a dataset as a matrix by simply \"stacking\" the input vectors ($x$) together.\n",
        "\n",
        "Let's look at the diamond dataset as a matrix by checking its shape. The input/feature matrix $X$ has 5000 examples each with an input vector of length 6. The 6 elements in the vector correspond to the 6 features that describe the attributes of the diamond. The output values are stored in $y$ as a vector of 5000 since there is only one output value per example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93-w23a0LRJr"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOqHs7owLhSU"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGpCfKrqKSjZ"
      },
      "source": [
        "Let's quickly check some statistics and basic facts about the dataset which are important for supervised learning. First, let's check the examples and features in our dataset. When working with data for ML tasks, we typically desire each feature variable to have a mean of 0 and a standard deviation of 1. For this project, we have already preprocessed the diamond dataset for you, so the features' means and standard deviations approximately match these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_tSOcTRBN0i"
      },
      "outputs": [],
      "source": [
        "# Check range, min/max, mean, std, etc. for all input feature columns\n",
        "pd.DataFrame(data=X).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2bArgfLGvuk"
      },
      "source": [
        "For regression, we typically map a real-valued input vector to a real-valued scalar (or vector). Remember that a scalar is a single value and a vector has multiple values. Many of the techniques we discuss here extend to _multi-variate_ regression where $y$ is a vector.\n",
        "\n",
        "When working with a dataset, we typically want to understand the rough *distribution* of the output variable $y$. First, let's plot the values of the output variable for our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8fMYKYZYXNH"
      },
      "outputs": [],
      "source": [
        "# scatterplot of y values with jitter\n",
        "plt.boxplot(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A6bgIPQ5bGr"
      },
      "source": [
        "**NOTE**: This dataset is already sufficiently pre-processed to use directly with our ML modeling algorithms. Typically when preparing a dataset for ML modeling we need to scale the values of the input and output vectors. In future datasets and projects we will learn about and apply such pre-processing and dataset standardization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-YThqSE6d-T"
      },
      "source": [
        "# Building and evaluating a first regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZZGq2tt8Mtq"
      },
      "source": [
        "We have now loaded the dataset and performed some basic data checks. Note that we are skipping dataset definition and data cleaning to focus this initial project on ML modeling work only.\n",
        "\n",
        "We will first establish metrics for our model before trying to improve it. This approach ensures that we are clear about what we are measuring before spending time building and improving models.\n",
        "\n",
        "Some setup steps:\n",
        "1. Evaluation metric selection and evaluation code.\n",
        "2. Split data into training / development sets.\n",
        "3. Build and evaluate simple baselines.\n",
        "\n",
        "And with our baselines established, we will move to iterative, hypothesis-driven modeling improvements:\n",
        "1. Build and evaluate a first regression model on the data.\n",
        "2. Diagnose next steps for how to improve model.\n",
        "3. Propose hypothesis for how to improve model and iterate!\n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIxkii6L8M38"
      },
      "source": [
        "## Evaluation metric selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbUX1I4BggkU"
      },
      "source": [
        "When working with regression, we typically default to mean squared error (MSE) as a guiding metric because it works well with many regression models. However, it's useful to consider multiple evaluation metrics during development so we can monitor for unusual model behavior, and make more comprehensive results comparisons.\n",
        "\n",
        " Let's introduce additional metrics to give a different view on performance with less sensitivity to outliers.\n",
        "\n",
        "To focus our initial work on model development, we will focus on MSE as a standard choice for evaluation, but track the others to check for unusual behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK1bxqz5IAwD"
      },
      "source": [
        "### Evaluation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLYmi3f8m-Z"
      },
      "source": [
        "To ensure we measure and compare models in a consistent way, we will create a shared function we can use to evaluate all models. Creating and validating this code with a simple baseline before building models gives us a controlled way to test and debug our metrics code before introducing the additional uncertainty of model results.\n",
        "\n",
        "This function is fairly simple, given actual and predicted values for a regression task, we apply multiple `sklearn.metrics` functions to compute error terms and return 3 values (MSE, MAE, MPE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrIXf9Kf7mBT"
      },
      "outputs": [],
      "source": [
        "def compute_eval(actual: np.ndarray, pred: np.ndarray ) -> Tuple[float, float, float]:\n",
        "  \"\"\"Computes evaluation metrics on given predicted/actual values\n",
        "  Args:\n",
        "    actual: 1-D array of actual values\n",
        "    pred: 1-D array of predicted values\n",
        "\n",
        "  Returns:\n",
        "    Mean squared error (float),\n",
        "    Mean absolute error (float),\n",
        "    Mean percentage error (float)\n",
        "\n",
        "  \"\"\"\n",
        "  return (mean_squared_error(actual, pred), mean_absolute_error(actual, pred),\n",
        "   mean_absolute_percentage_error(actual, pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFEIZPW8IO7U"
      },
      "source": [
        "### Evaluation unit test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw9urD4CZaz1"
      },
      "source": [
        "Now let's quickly check our evaluation function using some basic inputs. Think of this as a small software _[unit test](https://en.wikipedia.org/wiki/Unit_testing)_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LaqTtsFZvYq"
      },
      "outputs": [],
      "source": [
        "n_samples_test = 100\n",
        "print(\"Expect some error (MSE 2.25 MAE 1.5 MPE 1.0)\")\n",
        "print(\"Computed error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(1.5 * np.ones(n_samples_test), np.zeros(n_samples_test)))\n",
        "print(\"Expect 0 error\")\n",
        "print(\"Computed error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(np.ones(n_samples_test), np.ones(n_samples_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzXeh2UVYOpv"
      },
      "source": [
        "In these project notebooks, we provide several unit tests using a Python testing framework called `ipytest`. The tests below check your evaluation function much like the cell above, and they output an error message if your code does not pass the tests. You are not required to look at the test code, but you should make sure your tests pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "95mTfvCTVTki"
      },
      "outputs": [],
      "source": [
        "#@title Test: Evaluation function\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "@pytest.mark.parametrize('actual, expected', [\n",
        "    (compute_eval(1.5 * np.ones(n_samples_test), np.zeros(n_samples_test)), (2.25, 1.5, 1.0)),\n",
        "    (compute_eval(np.ones(n_samples_test), np.ones(n_samples_test)), (0.0, 0.0, 0.0)),\n",
        "])\n",
        "def test_compute_eval(actual, expected):\n",
        "    actual_mse, actual_mae, actual_mpe = actual\n",
        "    expected_mse, expected_mae, expected_mpe = expected\n",
        "    assert actual_mse == expected_mse, '''Your MSE calculation is incorrect'''\n",
        "    assert actual_mae == expected_mae, '''Your MAE calculation is incorrect'''\n",
        "    assert actual_mpe == expected_mpe, '''Your MPE calculation is incorrect'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m94HSCh_WY_6"
      },
      "source": [
        "## Split data into training / development sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjSWXQ2C6WG7"
      },
      "source": [
        "Now, we reserve part of the data as a development set to keep fixed during our work. In this case we keep our split simple, we will randomly allocate 80% of the data to a training set, and reserve the remaining 20% for our development set.\n",
        "\n",
        "Note that we do not have a _test_ set in this project. Remember, a _development_ set is data we withold for regular evaluation of our model during development. A true _test_ set is data we use only once (or rarely) to evaluate a model and get a sense for how it would perform on truly unseen data. We do not need a true test set in this project to learn about building and evaluating models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnZVirMj6uMC"
      },
      "outputs": [],
      "source": [
        "X_train, X_dev, y_train, y_dev = train_test_split(X, y, train_size=0.8, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSmUANpx54Aj"
      },
      "source": [
        "## **Task: Simple baseline: Predict average value**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHaFiCoQ5va9"
      },
      "source": [
        "With our metrics function validated, let's establish simple baselines on our actual dataset. These baselines are useful for establishing a bound on performance -- a model capturing useful patterns in the data should outperform predicting the average output value for all examples. If models are unable to outperform these baselines, we would expect bugs in our code or issues in the dataset preventing useful learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnoDO85l5y5r"
      },
      "source": [
        "Compute the mean (average) output ($y$) value on the training set. Use this value as a prediction for each dev example. Report results using the evaluation function. Store the mean in a variable called `y_mean`, and store your dev set predictions in a variable called `y_mean_pred`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn9KMT2wrY5a"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "# Store your answers in the following variables:\n",
        "# y_mean = ...\n",
        "# y_mean_pred = ...\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSSqdyxtcX3B"
      },
      "source": [
        "*You should expect a dev set MSE of ~1.051, MAE of ~0.880, and MPE of ~0.114*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IzF53xMbdEq-"
      },
      "outputs": [],
      "source": [
        "#@title Test: Predict the average value\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "def test_mean():\n",
        "    assert pytest.approx(7.75, 0.02) == y_mean, '''Your mean y value is incorrect'''\n",
        "\n",
        "@pytest.mark.parametrize('actual, expected', [\n",
        "    (compute_eval(y_dev, y_mean_pred), (1.0506, 0.8799, 0.1146)),\n",
        "])\n",
        "def test_compute_eval(actual, expected):\n",
        "    actual_mse, actual_mae, actual_mpe = actual\n",
        "    expected_mse, expected_mae, expected_mpe = expected\n",
        "    assert pytest.approx(expected_mse, 0.002) == actual_mse, '''Your baseline MSE is incorrect'''\n",
        "    assert pytest.approx(expected_mae, 0.002) == actual_mae, '''Your baseline MAE is incorrect'''\n",
        "    assert pytest.approx(expected_mpe, 0.002) == actual_mpe, '''Your baseline MPE is incorrect'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPc4kkJfvYbx"
      },
      "source": [
        "## Training/fitting a model with scikit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq7Ri2rU6Cyk"
      },
      "source": [
        "With baselines and evaluation established, it's time to begin building models. Typically we start with a simpler model that is easier to understand. Based on performance of this initial model, we will make decisions about whether to try more complex models on this dataset.\n",
        "\n",
        "Linear regression is a reliable first choice since it is well-studied, relatively easy to interpret, and has many extensions we can use to improve performance in different situations. Now let's fit a model and generate predictions from it to measure performance relative to actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye89CQuPSC-K"
      },
      "outputs": [],
      "source": [
        "## Initialize empty model\n",
        "regr_lin = linear_model.LinearRegression()\n",
        "\n",
        "# The .fit() function trains/fits a model using the dataset provided\n",
        "regr_lin.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions using the model\n",
        "lin_train_pred = regr_lin.predict(X_train)\n",
        "\n",
        "# Evaluate model predictions vs actual value\n",
        "print(\"Training set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_train, lin_train_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X4TUzhY1_9d"
      },
      "source": [
        "## **Task: Evaluate initial model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwLawtKj2FKW"
      },
      "source": [
        "Above we fit a `LinearRegression` model on the training set, and measured performance of the model on the training set. Now it's your turn! Using the same model we fit above, generate predictions and report performance on the dev set by calling the `compute_eval` function. Store the dev set predictions in a variable `y_dev_lin_pred`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbbMQhHGg0if"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "# Store your answer in the following variable:\n",
        "# y_dev_lin_pred = ...\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8-Gk2W8pjiW"
      },
      "source": [
        "*You should expect a dev set MSE of ~0.067, MAE of ~0.202, and MPE of ~0.026.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTgGCtKjpfVm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test: Evaluate initial model\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "@pytest.mark.parametrize('actual, expected', [\n",
        "    (compute_eval(y_dev, y_dev_lin_pred), (0.0666, 0.2018, 0.02618)),\n",
        "])\n",
        "def test_compute_eval(actual, expected):\n",
        "    actual_mse, actual_mae, actual_mpe = actual\n",
        "    expected_mse, expected_mae, expected_mpe = expected\n",
        "    assert pytest.approx(expected_mse, 0.002) == actual_mse, '''Your linear model's MSE is incorrect'''\n",
        "    assert pytest.approx(expected_mae, 0.002) == actual_mae, '''Your linear model's is incorrect'''\n",
        "    assert pytest.approx(expected_mpe, 0.002) == actual_mpe, '''Your linear model's MPE is incorrect'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LYnvItcgyAu"
      },
      "source": [
        "# Use diagnostics to create a hypothesis for how to improve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXmb_Pb5Xdo"
      },
      "source": [
        "So far we built and evaluated a first linear model for this task. This process was fairly straightforward, but after fitting an initial model, we have many possible paths for what we might try next.\n",
        "\n",
        "Let's consider some diagnostic questions to determine the most promising next action to try:\n",
        "1. Is the model improving compared with simple baselines on both the training and dev sets?\n",
        "2. Is training set performance at/above goal performance for this system? Is training set performance saturated near zero error?\n",
        "3. How do training set and dev set results compare with each other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dibqxq9t5A_n"
      },
      "source": [
        "Think about your response to each question above. Next we will use these diagnostics to decide a next action. Here are reasonable responses to the questions above for our current experiment:\n",
        "1. _Yes. The model clearly outperforms our simple baselines, indicating the model learned some statistical patterns on the training set which were useful in making correct predictions on the dev set._\n",
        "1. _Training set performance is far from zero error, and the model does not appear to be fitting training set perfectly._\n",
        "1. _Training set performance is very close to dev set performance, and not meeting overall performance goals._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybFQg810cF8P"
      },
      "source": [
        "**Diagnosis**\n",
        "\n",
        "Based on our question responses, we might suspect the model is _high bias_ and _low variance_. We conclude this because the model has significant training set error and training/dev errors are closely matched. We also use the term _under-fitting_ to describe a model which performs poorly on the dev set and does not fit the training data as well as we hope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lhmxdAUh5WT"
      },
      "source": [
        "**Hypothesis**\n",
        "\n",
        "With our _high bias, low variance, under-fitting_ diagnosis, we can form a hypothesis about how to improve performance. When our model is high bias relative to the task, typically we want to find ways to build a _more expressive model_. In the case of linear regression, the linear assumption of the model might be too simple to fit the training data and capture useful patterns for this problem.\n",
        "\n",
        "We hypothesize a more expressive model will better fit the training set, and this improved performance will generalize to the dev set. So we want to try building higher variance models.\n",
        "\n",
        "Let's test this hypothesis by doing a next iteration of our modeling development process with more expressive, nonlinear models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMnKHwrriszv"
      },
      "source": [
        "# Iterating on model improvements: Nonlinear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZQDOYvUw8C0"
      },
      "source": [
        "## Nearest neighbors model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv_LuNKLi2tZ"
      },
      "source": [
        "For a first nonlinear model, we use the simple but powerful _nearest neighbors_. In scikit, this algorithm is available using the same standard regressor interface as our linear model. The class in scikit is`KNeighborsRegressor`.\n",
        "\n",
        "Nearest neighbors makes a prediction by finding the  $k$-nearest training set examples to an example we wish to make a prediction about. The output of nearest neighbors is usually an average of the training labels from these $k$ examples. This method allows for highly nonlinear predictions since the local average of small collections of points can vary.\n",
        "\n",
        "We can modify prediction behavior by changing the number of neighbor points and distance or weighting function used to compute predicitons.\n",
        "Nearest neighbors can be used for both classification and regression. Here the choice of how many neighbors to use ($k$) is a _hyperparameter_ of our model. We use \"hyper\" because such model options are one level up from the standard model parameters we find from training data using `.fit()`.\n",
        "\n",
        "For more see [wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) or [scikit documentation](https://scikit-learn.org/stable/modules/neighbors.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQjI8plv2VZl"
      },
      "source": [
        "### **Task: Build a K-nearest neighbors model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWy-6HJU2V-G"
      },
      "source": [
        "Fit a `KNeighborsRegressor` model and report evaluation results on the training and development sets using `compute_eval()`. Adjust model hyperparameter `n_neighbors` ($k$) to find the best dev set performance you can.\n",
        "\n",
        "**Extension**: If you're already familiar with the basics of using a nearest neighbors model, try exploring more hyperparameters to improve performance. You can refer to documentation for available options, but usually weighting function, distance metric, and number of points are most impactful on predictions. Store your final model in a variable `regr_kn_best`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiqsR4sl3Ij-"
      },
      "outputs": [],
      "source": [
        "## initialize empty model\n",
        "regr_kn = KNeighborsRegressor()\n",
        "# You may initialize a new model to change settings\n",
        "# Store your final model in a variable:\n",
        "regr_kn_best = None\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC2FMnOqtP0q"
      },
      "source": [
        "*You should ensure that your `KNeighborsRegressor` performs better than the linear baseline on the train and dev sets.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0G9NmFEtbOI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test: Build a KNN model\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "import sklearn\n",
        "\n",
        "def test_knn_model():\n",
        "    assert type(regr_kn_best) == sklearn.neighbors._regression.KNeighborsRegressor, \\\n",
        "    '''Your best model is not a KNeighborsRegressor'''\n",
        "\n",
        "def test_knn_train_metrics():\n",
        "    knn_mse, knn_mae, knn_mpe = compute_eval(y_train, regr_kn_best.predict(X_train))\n",
        "    lin_mse, lin_mae, lin_mpe = compute_eval(y_train, regr_lin.predict(X_train))\n",
        "    assert knn_mse < lin_mse, \\\n",
        "      '''Your KNN model's training MSE is worse than the linear baseline'''\n",
        "    assert knn_mae < lin_mae, \\\n",
        "      '''Your KNN model's training MAE is worse than the linear baseline'''\n",
        "    assert knn_mpe < lin_mpe, \\\n",
        "      '''Your KNN model's training MPE is worse than the linear baseline'''\n",
        "\n",
        "def test_knn_dev_metrics():\n",
        "    knn_mse, knn_mae, knn_mpe = compute_eval(y_dev, regr_kn_best.predict(X_dev))\n",
        "    lin_mse, lin_mae, lin_mpe = compute_eval(y_dev, regr_lin.predict(X_dev))\n",
        "    assert knn_mse < lin_mse, \\\n",
        "      '''Your KNN model's dev MSE is worse than the linear baseline'''\n",
        "    assert knn_mae < lin_mae, \\\n",
        "      '''Your KNN model's dev MAE is worse than the linear baseline'''\n",
        "    assert knn_mpe < lin_mpe, \\\n",
        "      '''Your KNN model's dev MPE is worse than the linear baseline'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xaBCG7KSwy3"
      },
      "source": [
        "## The importance of train / evaluation data separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns8vcgeHS_PB"
      },
      "source": [
        "When working in ML we DO NOT call `.fit()` on the development set. Remember we use the development set to approximate how the model will perform on new, unseen data. To see why this is so important, let's see what happens when we allow _data leakage_ -- training the model on data from the development set. The nearest neighbors model stores the data used in `.fit()` to make predictions, so it can very directly show the impact of training on data that we use in evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nczCC6DXTzT"
      },
      "source": [
        "### **Task: What happens if you use the development set in training?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhIoFWWLXc5B"
      },
      "source": [
        "Create a new nearest neighbors model and use the *development* set to fit your model. Report the dev set performance and compare it with the dev set performance you obtained above when properly holding out the development set from training.\n",
        "\n",
        "You should be able to get nearly perfect performance (0 error) on the dev set by using a small number of points to make each prediction ( controlled with the `n_neighbors` parameters). However, this gives an unrealistic picture of performance, and the dev performance you obtain by training on the dev set is not a good indicator of how the model would perform on new, unseen data. Store this model in a variable `regr_kn_dev`.\n",
        "\n",
        "**WARNING**: We never train on the development/test set in normal practice. This exercise will show you why we evaluate models on data witheld from training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NqBXRi0X-vW"
      },
      "outputs": [],
      "source": [
        "regr_kn_dev = None\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "# Store your model in the following variable:\n",
        "# regr_kn_dev = ...\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5LyVEo8e6OoC"
      },
      "outputs": [],
      "source": [
        "#@title Test: Development set in training\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "def test_knn_model():\n",
        "    assert type(regr_kn_dev) == sklearn.neighbors._regression.KNeighborsRegressor, \\\n",
        "    '''Your model is not a KNeighborsRegressor'''\n",
        "\n",
        "def test_knn_metrics():\n",
        "    knn_mse, knn_mae, knn_mpe = compute_eval(y_dev, regr_kn_dev.predict(X_dev))\n",
        "    assert 0.0 == round(knn_mse, 2), \\\n",
        "      '''Your improperly trained KNN model should have near 0 error'''\n",
        "    assert 0.0 == round(knn_mae, 2), \\\n",
        "      '''Your improperly trained KNN model should have near 0 error'''\n",
        "    assert 0.0 == round(knn_mpe, 2), \\\n",
        "      '''Your improperly trained KNN model should have near 0 error'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV03OYtVwjtN"
      },
      "source": [
        "We have clear baselines, metrics, and our goal is to find a nonlinear model which improves upon current performance. Let's try it!\n",
        "\n",
        "We often don't know in advance which type of nonlinear model will perform best on a given dataset. Instead, we focus on making it easy to compare several models and empirically discover which fits our dataset well.\n",
        "\n",
        "In the section below we will try a few different models to see if they work well on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93SkydNNwOMZ"
      },
      "source": [
        "## Decision tree model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg7gqMuVWKhF"
      },
      "source": [
        "Decision trees are a class of models which approximate functions by forming a set of rules or questions about a datapoint. For a quick overview of decision trees you can see the [scikit documentation](https://scikit-learn.org/stable/modules/tree.html). Let's build a first decision tree on our dataset and visualize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvP31HimXn3F"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "regr_dt = DecisionTreeRegressor()\n",
        "regr_dt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_train, regr_dt.predict(X_train)))\n",
        "\n",
        "print(\"Dev set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_dev, regr_dt.predict(X_dev)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPOU6pVuc7Vw"
      },
      "source": [
        "### Overfitting the training set and controlling decision tree complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl20qb4DZBDB"
      },
      "source": [
        "Notice the model obtains close to 0 error on the training set -- it fit the training data nearly perfectly! Unfortunately dev set performance is much **worse** than our linear model baseline. This is a classic example of  _overfitting_ the training set which is a technical term in ML for when a model fits the training set well, but learns a function which generalizes poorly to unseen data.\n",
        "\n",
        "Overfitting happens when our model is too high variance (expressive). It means the model is capable of fitting the training set very well, but fits the training set by finding a function that happens to only fit our training set well rather than all data we would feed to the model. We correct for overfitting by choosing simpler models, or biasing our model search/optimization procedure to prefer less expressive models.\n",
        "\n",
        "Decision tree models offer a few ways to easily limit model variance / complexity. We can restrict the depth of the decision tree using the `max_depth` parameter, or force the tree building algorithm to only allow leaf nodes when there is some minimum number of training examples corresponding to that leaf (a leaf is the lowest part of the tree after branching based on variable values as we go down the tree from the root) Let's try building a tree with a larger value of `min_samples_leaf`, the default value is 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u2Z-t2Be0IW"
      },
      "outputs": [],
      "source": [
        "regr_dt_simpler = DecisionTreeRegressor(min_samples_leaf=20)\n",
        "regr_dt_simpler.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_train, regr_dt_simpler.predict(X_train)))\n",
        "\n",
        "print(\"Dev set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_dev, regr_dt_simpler.predict(X_dev)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0kD1QCcJH1E"
      },
      "source": [
        "### Visualizing decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33opoUgfpujh"
      },
      "source": [
        "The visual plot of a decision tree expresses its set of rules for predicting values of new examples. We start at the top (root) of the tree, and check the value of the variable specified. We choose the left or right branch and repeat this process of checking values and branching left/right until we reach the bottom (leaf) of the tree. The model's predicted value is whatever that leaf specifies.\n",
        "\n",
        "The model plot is quite dense, don't worry about trying to read specifics of this model. This should give you a tangible sense of \"model complexity\" and some intuitions for how complex models can effectively memorize a training set in ways that don't generalize well to new data.\n",
        "\n",
        "OPTIONAL: If you adjust the parameters used in your decision tree you can visually observe differences in the complexity of the resulting tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nnfF9Wsf-NW"
      },
      "outputs": [],
      "source": [
        "tree.plot_tree(regr_dt_simpler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJZy6s0-yZX"
      },
      "source": [
        "### Model selection helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_0C6Uyw-8Ax"
      },
      "source": [
        "To find good settings for your nearest neighbors model, you likely ran multiple iterations of fitting the model and evaluating results. Overfitting with the decision tree shows why we need to carefully search over model parameters to build a good model.\n",
        "\n",
        "Let's search the space of possible models in a more principled way. We often frame the problem of ML model development as searching for the best model out of infinitely many possibilities.\n",
        "\n",
        "Below we provide a function which takes a list of models along with training and dev data. This function calls `.fit()` on each model with the training set, and then evaluates on both training and dev sets. The function returns a list of training and dev metric tuples corresponding to the model list input. This helper function allows us to quickly evaluate a set of candidate models as we search for the best model. It's okay if you can't follow all the Python code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOWBdb3_r54"
      },
      "outputs": [],
      "source": [
        "def search_models_helper(model_list: list, X_tr: np.ndarray, y_tr: np.ndarray,\n",
        "                        X_dev: np.ndarray, y_dev: np.ndarray) -> Tuple[list, list, int]:\n",
        "  \"\"\"Evaluate each model in model_list\n",
        "  Args:\n",
        "    model_list: 1-D array of scikit models to train with .fit()\n",
        "    X_tr: 2-D array of training set inputs\n",
        "    y_tr: 1-D array of training set labels\n",
        "    X_tr: 2-D array of dev set inputs\n",
        "    y_tr: 1-D array of dev set labels\n",
        "\n",
        "  Returns: Tuple() of:\n",
        "    dev_metrics_list: List of tuples for dev set metrics. Same order as model_list\n",
        "    train_metrics_list: List of tuples for training set metrics. Same order as model_list\n",
        "    best_mse_index: int model_list index of lowest dev set MSE model\n",
        "  \"\"\"\n",
        "  best_mse = Inf\n",
        "  best_idx = -1\n",
        "  dev_metrics_list = []\n",
        "  train_metrics_list = []\n",
        "  for cur_idx, cur_model in enumerate(model_list):\n",
        "    cur_model.fit(X_tr, y_tr)\n",
        "\n",
        "    # use evaluation function\n",
        "    train_metrics_list.append(compute_eval(y_tr, cur_model.predict(X_tr)))\n",
        "    dev_metrics_list.append(compute_eval(y_dev, cur_model.predict(X_dev)))\n",
        "\n",
        "    # check if new best model\n",
        "    cur_mse = dev_metrics_list[-1][0]\n",
        "    if cur_mse < best_mse:\n",
        "      best_idx = cur_idx\n",
        "      best_mse = cur_mse\n",
        "\n",
        "  return (dev_metrics_list, train_metrics_list, best_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkNz3AQKwWRo"
      },
      "source": [
        "### **Task: Try several decision tree models to obtain better performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPRiZdQ3lmhx"
      },
      "source": [
        "Below is an example of using the model selection helper to try different settings for the decision tree model. Modify the list of models to try a larger set of possible decision tree models. Store your best model in the `regr_dt_best` variable.\n",
        "\n",
        "Expected result: In our experiments the decision tree model tends to perform worse on the dev set than our linear baseline. You should try some additional models and achieve dev set MSE worse than the linear baseline, but better than our baseline of predicting the mean value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNIYwiig5T-J"
      },
      "outputs": [],
      "source": [
        "## Store your best model in this variable:\n",
        "regr_dt_best = None\n",
        "# List of models to try. Add more models to explore hyperparameter settings.\n",
        "model_list = []\n",
        "model_list.append(DecisionTreeRegressor(max_depth=2))\n",
        "model_list.append(DecisionTreeRegressor(max_depth=10, min_samples_split=50))\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCZrIIIoBecC"
      },
      "source": [
        "*You should ensure that your decision tree model has better training set performance than the linear baseline.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIdww2rp_UfK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test: Decision tree modeling\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "def test_dt_model_class():\n",
        "    assert type(regr_dt_best) == sklearn.tree._classes.DecisionTreeRegressor, \\\n",
        "    '''Your best model is not a DecisionTreeRegressor'''\n",
        "\n",
        "def test_dt_train_metrics():\n",
        "    dt_mse, dt_mae, dt_mpe = compute_eval(y_train, regr_dt_best.predict(X_train))\n",
        "    lin_mse, lin_mae, lin_mpe = compute_eval(y_train, regr_lin.predict(X_train))\n",
        "    assert dt_mse < lin_mse, \\\n",
        "      '''Your DT model's training MSE is worse than the linear baseline'''\n",
        "    assert dt_mae < lin_mae, \\\n",
        "      '''Your DT model's training MAE is worse than the linear baseline'''\n",
        "    assert dt_mpe < lin_mpe, \\\n",
        "      '''Your DT model's training MPE is worse than the linear baseline'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xS7fB7v1am7"
      },
      "source": [
        "## **Optional: Try additional nonlinear algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUnkWiHl1o1o"
      },
      "source": [
        "Nearest neighbors is an intuitive ML algorithm since predictions are based on values from the training set near a query point. Below are some additional modeling approaches that you can try, but you might need to understand more about how the models work to achieve good results. To learn the intuition for any model listed here, you can start with the scikit documentation.\n",
        "\n",
        "See if you can obtain better dev set performance with a nonlinear model we have not yet tried. Use the search models helper function to try several hyperparameter settings. Report the lowest dev MSE you find and describe the model settings which produced it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riQOChK11ZPu"
      },
      "outputs": [],
      "source": [
        "# Random forest\n",
        "# Try adjusting n_estimators and max_leaf_nodes\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# RandomForestRegressor(n_estimators=20, max_leaf_nodes=2)\n",
        "\n",
        "# Gradient boosting regressor\n",
        "# Try adjusting n_estimators and max_leaf_nodes\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Multi-layer perceptron (Simple neural network)\n",
        "# Try adjusting hidden_layer_sizes\n",
        "# from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rjb0pq-o2hg"
      },
      "source": [
        "## **Plotting performance as a function of model parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2IPM0OMuA9"
      },
      "source": [
        "It's sometimes helpful to create a plot to show the comparative MSE performance of several models. Here we demonstrate how to plot the performance of nearest neighbors models as we vary $k$, the number of neighbors used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FknfxBh6d-FI"
      },
      "outputs": [],
      "source": [
        "# create a list of nearest neighbor models\n",
        "kn_choices = range(2,50,2)\n",
        "model_list = []\n",
        "for k in kn_choices:\n",
        "  model_list.append(KNeighborsRegressor(n_neighbors=k))\n",
        "\n",
        "dev_metrics_list, train_metrics_list, best_idx = search_models_helper(model_list, X_train, y_train, X_dev, y_dev)\n",
        "\n",
        "# plot resulting performance\n",
        "plt.plot(kn_choices, [ev[0] for ev in dev_metrics_list], 'kx', label='Dev')\n",
        "plt.plot(kn_choices, [ev[0] for ev in train_metrics_list], 'ko', label='Train')\n",
        "plt.title('MSE for Nearest Neighbor Models')\n",
        "plt.xlabel('n_neighbors')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkCTRe584L90"
      },
      "source": [
        "### **Optional: Plot performance of models you searched over**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hbO9VpNkn43"
      },
      "source": [
        "Create a new plot to display performance vs hyperparameter values for any of the other nonlinear models you tried. This can help build intuition for how different options in a nonlinear model affect train and dev set performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo3HR8rRnirw"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHHcUjI4i2bM"
      },
      "source": [
        "## **Optional: Hyperparameter search and grid search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-FdccWAjFTg"
      },
      "source": [
        "For a more advanced approach to searching for good hyperparameter settings, try defining your list of potential models using a principled hyperparameter search technique. For example, you can specify the minimum and maximum values for each hyperparameter, and then try models for many combinations of values within the ranges. Choosing values at fixed intervals is simple and often effective, it's called _grid search_. You can also try more sophisticated hyperparameter optimization algorithms like [hyperopt](http://hyperopt.github.io/hyperopt/), however these are mostly used when trying to build large models or fully automate model selection.\n",
        "\n",
        "Below we show an example of using sckit gridsearch for a decision tree model. You can read scikit's [best practices page](https://scikit-learn.org/stable/modules/grid_search.html) for more on how you might set up search for a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwxe1VQpno1V"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "regr_dt = DecisionTreeRegressor(random_state=0)\n",
        "param_grid = {'max_depth': [2, 5, 10],\n",
        "              'min_samples_split': [2,10,30,50]}\n",
        "dt_gs = GridSearchCV(regr_dt, param_grid, cv=5).fit(X_train, y_train)\n",
        "print(dt_gs.best_estimator_)\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKdU1QF6NxqG"
      },
      "source": [
        "# Diagnose next steps after nonlinear modeling improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAVMY_CmOu-2"
      },
      "source": [
        "After iterating on nonlinear modeling improvements, let's compare results of the best nonlinear model you found with our initial linear model from earlier. We will evaluate our hypothesis that a higher variance, nonlinear model can better fit the underlying function corresponding to this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQsxj9tqZXd"
      },
      "source": [
        "## **Task: Select your best model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhw0iLaFqgUB"
      },
      "source": [
        "For the various nonlinear models you tried above, choose one that achieves the best dev set error you were able to obtain. Ensure you fit it to the training set, and store it in `regr_nl_best`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tURBYl_pn1go"
      },
      "outputs": [],
      "source": [
        "regr_nl_best = None\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5roM697GB-XD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test: Select your best model\n",
        "\n",
        "%%ipytest\n",
        "\n",
        "import sklearn\n",
        "\n",
        "def test_nl_model():\n",
        "    assert regr_nl_best is not None and \\\n",
        "      type(regr_nl_best) != sklearn.linear_model._base.LinearRegression, \\\n",
        "    '''Your best model is not a KNeighborsRegressor'''\n",
        "\n",
        "def test_nl_train_metrics():\n",
        "    nl_mse, nl_mae, nl_mpe = compute_eval(y_train, regr_nl_best.predict(X_train))\n",
        "    lin_mse, lin_mae, lin_mpe = compute_eval(y_train, regr_lin.predict(X_train))\n",
        "    assert nl_mse < lin_mse, \\\n",
        "      '''Your nonlinear model's training MSE is worse than the linear baseline'''\n",
        "    assert nl_mae < lin_mae, \\\n",
        "      '''Your nonlinear model's training MAE is worse than the linear baseline'''\n",
        "    assert nl_mpe < lin_mpe, \\\n",
        "      '''Your nonlinear model's training MPE is worse than the linear baseline'''\n",
        "\n",
        "def test_nl_dev_metrics():\n",
        "    nl_mse, nl_mae, nl_mpe = compute_eval(y_dev, regr_nl_best.predict(X_dev))\n",
        "    lin_mse, lin_mae, lin_mpe = compute_eval(y_dev, regr_lin.predict(X_dev))\n",
        "    assert nl_mse < lin_mse, \\\n",
        "      '''Your nonlinear model's dev MSE is worse than the linear baseline'''\n",
        "    assert nl_mae < lin_mae, \\\n",
        "      '''Your nonlinear model's dev MAE is worse than the linear baseline'''\n",
        "    assert nl_mpe < lin_mpe, \\\n",
        "      '''Your nonlinear model's dev MPE is worse than the linear baseline'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yCf_gX1wqD1"
      },
      "source": [
        "## Nonlinear modeling hypothesis evaluation. Did it work?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu53zXEXOjji"
      },
      "outputs": [],
      "source": [
        "print(\"Linear model:: \")\n",
        "print(\"Training set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_train, regr_lin.predict(X_train)))\n",
        "\n",
        "print(\"Dev set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_dev, regr_lin.predict(X_dev)))\n",
        "\n",
        "print(\"Best nonlinear model:: \")\n",
        "print(\"Training set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_train, regr_nl_best.predict(X_train)))\n",
        "\n",
        "print(\"Dev set error: MSE: %.5f  MAE: %.5f  MPE: %.5f \"\n",
        "      % compute_eval(y_dev, regr_nl_best.predict(X_dev)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKAZKDMRlbOK"
      },
      "source": [
        "**Results check**: You should have found at least one nonlinear model which obtains lower dev set MSE than the linear model baseline. For reference, a nearest neighbors model with good setting for $k$ can obtain $ MSE < 0.065$ on the dev set.\n",
        "\n",
        "Recall our hypothesis that _a more expressive model will better fit the training set, and this improved performance will generalize to the dev set._ In terms of MSE we have validated this hypothesis! Now we have a nonlinear model which generates better aggregate predictions than our initial model. Let's evaluate predictions from our nonlinear model to help form a hypothesis for how to next improve our system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RR6kc5Gw5MG"
      },
      "source": [
        "## Visualizing linear vs nonlinear model predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woVjEDcLy_Kz"
      },
      "source": [
        "Some additional plotting and diagnostics can help us compare performance between models.\n",
        "First we can check the _predicted_ vs _actual_ values for $y$ for both models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8gEXFWL0zXV"
      },
      "source": [
        "### Predicted vs actual values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGM3dGNS0vF4"
      },
      "source": [
        "This scatter plot shows us where model predictions disagree with ground truth, perfect predictions are along the diagonal line $y=x$. This plot helps us check for patterns of errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrT0CWhZR83s"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_dev, regr_lin.predict(X_dev), 'bo', alpha=0.2, label='linear')\n",
        "plt.plot(y_dev, regr_nl_best.predict(X_dev), 'ro', alpha=0.2, label='nonlinear')\n",
        "plt.plot(y_dev, y_dev, 'k-', label='correct')\n",
        "plt.ylim(np.min(y_dev), np.max(y_dev))\n",
        "plt.xlabel(\"Actual \")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title('Predicted vs Actual Dev Set')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBqplsoPywf3"
      },
      "source": [
        "**Results check**: Depending on the nonlinear model you chose, you might observe more or less systematic differences in the prediction comparison plot. Generally on this dataset you should be able to obtain a nonlinear model which performs better in aggregate than a nonlinear model, and does not exhibit clearly biased predictions in the comparison plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VachN4-v1BXZ"
      },
      "source": [
        "### Model prediction disagreement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC628oO5bGUE"
      },
      "source": [
        "To more directly understand where predictions differ, we can construct a similar plot using the linear model's predictions as $x$ axis, and your nonlinear model's predictions as $y$ axis. This plot helps us investigate the question, _\"How do predictions from the nonlinear model deviate from linear model predictions?_\"\n",
        "\n",
        "* Note this does not say whether either model is more correct, just where the predictions differ.\n",
        "* The line $y=x$ in this plot is where both models predict the same value for a dev set example.\n",
        "* Points above the line $y=x$ indicate the nonlinear model predicting a larger value compared with the linear model.\n",
        "* This diagnostic can help us check for unusual bias or outliers in a more complex model. This helps guide decisions about which model to use in a production system.\n",
        "* If a nonlinear model is better overall but exhibits non-uniform over/under-estimation compared with the linear model, you might want to further investigate nonlinear model predictions before putting the model into production\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJv3tNfsbvFC"
      },
      "outputs": [],
      "source": [
        "plt.plot(regr_lin.predict(X_dev), regr_nl_best.predict(X_dev), 'bo', alpha=0.2)\n",
        "\n",
        "plt.plot(regr_lin.predict(X_dev), regr_lin.predict(X_dev), 'k-', label='$y=x$')\n",
        "plt.ylim(np.min(y_dev), np.max(y_dev))\n",
        "plt.xlabel(\"Linear\")\n",
        "plt.ylabel(\"Nonlinear\")\n",
        "plt.title('Comparing predicted values $y$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU5l3u76zKef"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke5vaOnBzVze"
      },
      "source": [
        "Let's review what we have learned from our ML experiments thus far:\n",
        "\n",
        "* Nonlinear models yielded some improvement on the development set, and our visualizations suggest that nonlinear models are making more accurate predictions without obviously biased behavior compared with the linear model baseline.\n",
        "* Many nonlinear models can achieve low error on the training set, but improvements do not generalize to the dev set. _Often this is an indicator that the training dataset might not have enough examples or overall variation to warrant more complex models_.\n",
        "* We coarsely explored different types of nonlinear models, and we can likely further tune our best nonlinear models if we want to improve performance a bit further\n",
        "\n",
        "**Diagnosis**: Reaching diminishing returns of performance improvement as we try different nonlinear model choices. This is a situation we frequently encounter in ML modeling work: nonlinear models are able to fit the variation in our training set and achieve low/no error on the training set, however the nonlinear models fail to perform well on the development set which suggests the more complex learned function is finding artifacts of the training set rather than useful, generalizable variations in the data. _Overall, we conclude in this situation that our nonlinear models are helping a bit, but we are nearing the end of improvements we can achieve by model changes. Nonlinear models might already be fitting artifacts in the training set and failing to generalize beyond our current performance._\n",
        "\n",
        "**A note on regularization**: Nonlinear models are _high variance_ and can approximate complex functions. We needed to slightly _reduce_ the variance of our models by choosing simpler versions of nonlinear models to avoid overfitting. Biasing our model search to prefer simpler models is a form of _regularization_. While the math varies depending on the models used, regularization broadly refers to biasing our model search or optimization procedure to prefer certain types of models -- often simpler models. Often times regularization is especially important on smaller datasets, or when the number of input features is large compared with the number of training examples available. We will further explore regularization in upcoming projects.\n",
        "\n",
        "_Congratulations on completing the project!_ You tried several nonlinear regression models and built a complete initial ML modeling experiment. In this project, we introduced several regression techniques, error metrics, hypothesis-driven development, and regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSlubaQts-QR"
      },
      "source": [
        "# Optional: Creating derived features and linear regression regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38UWjzsxkWBF"
      },
      "source": [
        "## Improving a linear model with derived features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K8QVYLkt4nn"
      },
      "source": [
        "If a linear model is not sufficiently fitting our training set, we might improve by introducing _new input features_. Adding new input features allows us to fit new linear models on an expanded feature set which hopefully better fit the data.\n",
        "\n",
        "We create a new input feature vector $\\tilde{x} = [x, x_{new}]$ which includes our original features $x$ as well as new features $x_{new}$. Adding new input features often happens as we add new information to improve predictions when _under-fitting_. In ML engineering work, we often create new features by understanding a task or domain, and ensuring all relevant information is available to the ML model (e.g. by having insights on diamond quality and ensuring we have those attributes as inputs to a model).\n",
        "\n",
        "Rather than expanding $x$ with new information about the task, for now we will make our linear model more expressive by expanding $x$ with _derived features_.\n",
        "Our new input features will be nonlinear transformations of the existing $x$. Scikit provides a simple _transform_ mechanism which produces new input features $\\tilde{x}$ for all examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV7A9Qglh2CN"
      },
      "outputs": [],
      "source": [
        "# perform a polynomial features transform of the dataset\n",
        "trans = PolynomialFeatures(degree=2)\n",
        "X_train_trans = trans.fit_transform(X_train)\n",
        "X_dev_trans = trans.fit_transform(X_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjDlvnNcCD-A"
      },
      "source": [
        "A degree 2 polynomial transform means if we had original input features $x=[a, b]$, the derived input features are $\\tilde{x} = [1, a, b, a^2, ab, b^2]$. We can extend this to any polynomial order >= 2, but this can quickly grow the number of features too large for easy use.\n",
        "\n",
        "\n",
        "Recall a linear model takes the form $y = \\theta_1 x_1 + \\theta_2 x_2 ...$ so applying a linear model to our derived features allows us to model some nonlinear variation in our original input data because we now have a _linear_ model applied to _nonlinear_ derived features of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgVpvOiIbJ8X"
      },
      "source": [
        "### **Optional: Evaluate and compare a linear model with derived features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvg62uydhIn6"
      },
      "source": [
        "Fit and evaluate a linear regression model on the transformed training and dev sets just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huxTG2TULa4B"
      },
      "outputs": [],
      "source": [
        "# fit an evaluate a linear model on the transformed features\n",
        "regr_poly_lin = linear_model.LinearRegression()\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUxv_E-ZkIYp"
      },
      "source": [
        "The expanded features model has _lower bias_ and _higher variance_ compared to the model with original features. We conclude this because the new model has more free parameters ($\\theta$), and can model some nonlinear relations in the data via the derived features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdl-YWsmp3DG"
      },
      "source": [
        "## Linear regression with regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ry1P25MTef_"
      },
      "source": [
        "_Regularization_ introduces a penalty for model complexity when fitting/training the model. By introducing such a penalty, regularization provides a way to trade off model complexity (variance) with how well a model generalizes to perform well on dev set examples.\n",
        "\n",
        "We typically add a regularization term to the loss function so that our final model will be influenced by the regularization penalty and our original objective/loss function. We now think of our overall loss function $\\mathcal{L}_{\\mathcal{D}}^{tot}$ as the sum of our original loss function and a new loss term $\\mathcal{L}_{\\mathcal{D}}^{reg}$,\n",
        "\n",
        "$\\mathcal{L}_{\\mathcal{D}}^{tot} (f_\\theta(x)) =\n",
        "\\mathcal{L}_{\\mathcal{D}}^{fit} (f_\\theta(x))\n",
        "+ \\alpha \\mathcal{L}_{\\mathcal{D}}^{reg} (\\theta)$.\n",
        "\n",
        "The parameter $\\alpha$ allows us to adjust the trade-off between the two terms. Directly controlling this tradeoff allows us to penalize the complexity of an over-fitting model and hopefully improve dev set performance by preventing the model from fitting unhelpful relationships in the training set.\n",
        "\n",
        "An often used, simple regularization approach is sum of squared values of the model parameters $\\theta$,\n",
        "\n",
        "$\\mathcal{L}_{\\mathcal{D}}^{reg} (\\theta) = \\alpha \\sum_j \\theta_j^2$\n",
        "\n",
        "This sum of squares regularization is also called $L_2$ since it corresponds to the $L_2$ norm of the vector $\\theta$.\n",
        "\n",
        "Finally, let's see our full linear regression loss with $L_2$ regularization,\n",
        "\n",
        "$\\mathcal{L}_{\\mathcal{D}} (f_\\theta(x)) = \\sum_{i \\in \\mathcal{D}} (\\hat{y}^{(i)} - y^{(i)})^2 +\n",
        "\\alpha \\sum_j \\theta_j^2$\n",
        "\n",
        "This regression and regularization combination is well-studied and widely used. In fact, it has a special name, ridge regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdz5YaAQc8Ye"
      },
      "source": [
        "### **Optional: Find a good regularization setting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpfbNgW_dC7F"
      },
      "source": [
        "Using the `Ridge` model and your `search_models_helper()` function, try several settings for the regularization parameter $\\alpha$. Report training and dev set metrics for the best model you find for *both* the expanded features and original datasets.\n",
        "\n",
        "Fit and store the best model for each feature set. Report the training and dev set metrics of the best model.\n",
        "\n",
        "Expected result: Regularization should help you improve a bit over the linear baseline model with no regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI97V4iEhYNW"
      },
      "outputs": [],
      "source": [
        "# try at least these values but you can add more\n",
        "alpha_list = [0, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "## store your lowest MSE model in the variable below\n",
        "regr_ridge_ = None\n",
        "\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "\n",
        "#############################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DX3VPazF8AGN",
        "UFEIZPW8IO7U",
        "SkCTRe584L90",
        "xHHcUjI4i2bM",
        "FKdU1QF6NxqG",
        "3QQsxj9tqZXd",
        "_yCf_gX1wqD1",
        "0RR6kc5Gw5MG",
        "X8gEXFWL0zXV",
        "VachN4-v1BXZ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}